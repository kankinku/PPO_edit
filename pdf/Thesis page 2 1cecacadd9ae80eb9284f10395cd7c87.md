# Thesis page 2

---

## ğŸ“˜ 2. Policy Optimization (ì •ì±… ìµœì í™”)

### 2.1 Policy Gradient Methods (ì •ì±… ê²½ì‚¬ ë°©ë²•)

- **ì •ì±… ê²½ì‚¬ë²•**ì€ ì •ì±…ì˜ gradient(ê¸°ìš¸ê¸°) ì¶”ì •ì¹˜ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ í†µí•´ í™•ë¥ ì  ê²½ì‚¬ ìƒìŠ¹(gradient ascent)ì„ ìˆ˜í–‰í•˜ì—¬ ì •ì±…ì„ ìµœì í™”í•¨.
- ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” gradient ì¶”ì • ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŒ:
    
    $$
    \hat{g} = \mathbb{E}_t\left[\nabla_\theta \log \pi_\theta(a_t \mid s_t)\hat{A}_t \right]
    $$
    
    ---
    
    $$
    \pi_\theta
    $$
    
    ^ í™•ë¥ ì  ì •ì±… (state sts_tì—ì„œ action ata_të¥¼ ì„ íƒí•  í™•ë¥ )
    
    $$
    \hat{A}_t
    $$
    
    ^  Advantage functionì˜ ì¶”ì •ê°’
    
    $$
    \mathbb{E}_t
    $$
    
    ^ ìƒ˜í”Œì— ëŒ€í•œ ê²½í—˜ì  í‰ê·  (sampling â†’ optimization ë°˜ë³µ)
    
    ---
    
- Objective function:
    
    $$
    L^{PG}(\theta) = \mathbb{E}_t\left[\log \pi_\theta(a_t \mid s_t)\hat{A}_t \right]
    $$
    
    ^ ì´ í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•´ì„œ ì •ì±… gradientë¥¼ ì–»ìŒ.
    
- **ì£¼ì˜ì‚¬í•­**: ê°™ì€ trajectory(ê²½ë¡œ)ë¡œ ì—¬ëŸ¬ ë²ˆ ì´ objectiveë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì€ ì˜ëª»ëœ ë°©ì‹ì¼ ìˆ˜ ìˆìœ¼ë©°, ë„ˆë¬´ í° policy ë³€í™”ë¡œ ì¸í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŒ.

---

### 2.2 Trust Region Methods (ì‹ ë¢° ì˜ì—­ ê¸°ë°˜ ë°©ë²•)

- **TRPO (Trust Region Policy Optimization)**: ì •ì±… ì—…ë°ì´íŠ¸ ì‹œ ì •ì±…ì´ ì§€ë‚˜ì¹˜ê²Œ ë³€í•˜ì§€ ì•Šë„ë¡ ì œí•œì„ ë‘ëŠ” ë°©ì‹.
- ìµœì í™” ëª©ì :
    
    $$
    \max_{\theta} \mathbb{E}_t\left[\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)}A_t \right]
    $$
    
- ì œì•½ì¡°ê±´:
    
    $$
    \mathbb{E}_t[\text{KL}[\pi_{\theta_{\text{old}}}(\cdot \mid s_t), \pi_\theta(\cdot \mid s_t)]] \leq \delta
    $$
    
    â†’ KL divergenceê°€ ì‘ë„ë¡ (ì •ì±…ì´ í¬ê²Œ ë³€í•˜ì§€ ì•Šë„ë¡) ì œí•œ
    
- ì´ ë¬¸ì œëŠ” ì„ í˜• ê·¼ì‚¬ì™€ 2ì°¨ ê·¼ì‚¬ë¥¼ í†µí•´ conjugate gradient ë°©ë²•ìœ¼ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŒ.
- **ì‹¤ì œë¡œëŠ” penalty ë°©ì‹**ë„ ì‚¬ìš©í•¨:
    
    $$
    \max_{\theta} \mathbb{E}_t\left[\frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_{\text{old}}}(a_t \mid s_t)} \hat{A}_t - \beta \text{KL}[\pi_{\theta_{\text{old}}}(\cdot \mid s_t), \pi_\theta(\cdot \mid s_t)] \right]
    $$
    
    - Î² :  KL íŒ¨ë„í‹° ê³„ìˆ˜
    - KL divergenceì— penaltyë¥¼ ì£¼ëŠ” ë°©ì‹ì€ ë” ìœ ì—°í•˜ë‚˜, ì¢‹ì€ Î²\beta ê°’ì„ ì°¾ëŠ” ê²ƒì´ ì–´ë µê¸° ë•Œë¬¸ì— TRPOëŠ” ì¼ë°˜ì ìœ¼ë¡œ hard constraint(ì œì•½) ë°©ì‹ì„ ì±„íƒ.
    

---

### ğŸ”‘ ìš”ì•½

- **Policy Gradient**: ì •ì±… ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ì„œ ìµœì í™”í•˜ëŠ” ê¸°ë³¸ ë°©ì‹. ê°„ë‹¨í•˜ì§€ë§Œ í° ì—…ë°ì´íŠ¸ì— ì·¨ì•½í•¨.
- **TRPO**: ì •ì±…ì´ ë„ˆë¬´ ê¸‰ê²©íˆ ë³€í•˜ì§€ ì•Šë„ë¡ ì œí•œ(KL ì œí•œ)ì„ ë‘ì–´ ì•ˆì •ì„±ì„ ë†’ì„.
- íŒ¨ë„í‹° ë°©ì‹ë„ ì¡´ì¬í•˜ì§€ë§Œ Î²\beta ì¡°ì •ì´ ì–´ë µê¸° ë•Œë¬¸ì— ì¼ë°˜ì ìœ¼ë¡œ TRPOëŠ” hard constraintë¥¼ ì„ í˜¸í•¨.

---